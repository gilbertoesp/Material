{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Universidad de Sonora](http://www.identidadbuho.uson.mx/assets/letragrama-rgb-72.jpg)\n",
    "## Ciencias de la Computación\n",
    "### [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Predicción en redes neuronales: El algoritmo de *feedforward*\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 24 de agosto de 2017.\n",
    "\n",
    "En esta libreta vamos a practicar con el algoritmo básico para realizar reconocimiento en redes neuronales hacia adelante y establecer una estructura básica para simular cn fines de comprensión. Para aplicaciones reales vamos autilizar herramientas poderosas como [Tensorflow](https://www.tensorflow.org) o [Theano](http://www.deeplearning.net/software/theano/), pero es importante hacer una primer red neuronal simple a pie con el objetivo de entender mejor los mecanismos básicos.\n",
    "\n",
    "Como dijo Jack el destripador, vamos por partes, y empecemos con asumir que tenemos la especificación completa de la red neuronal y lo que queremos es poder generar una red neuronal inicial, o poder recuperar una red existente previamente guardada.\n",
    "\n",
    "Empecemos por inicializar los modulos que vamos a requerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Especificando una red neuronal\n",
    "\n",
    "Primero, para poder hacer una red neuronal, tenemos que determinar cierta información. La información importante que debemos de especificar cuando hacemos una redes neuronales es:\n",
    "\n",
    "- Cuantas capas de neuronas tiene la red neuronal, $L$.\n",
    "- Cuantas neuronas va a tener cada capa $[n_0, n_1, \\ldots, n_L]$, donde $n_0$ es el número de entradas y $n_L$ el número de salidas.\n",
    "- Cual es la función de activación de las neuronas ocultas (logística, lineal rectificada, ...).\n",
    "- Cual es el tipo de salida de mi red neuronal (lineal, logística, unidad softmax, ... ).\n",
    "- Los valores con los que se normalizan los datos de entrada a la red neuronal (para el aprendizaje en una red neuronal es muy importante que los valores de entrada estén normalizados).\n",
    "\n",
    "Una vez que se establecen estos valores, es necesario generar una lista de matrices $[W^{(1)}, \\ldots, W^{(L)}]$ donde $W^{(l)}$ es una matriz de dimensiones $(n_l, n_{l-1} + 1)$ de parámetros o pesos. Si se inicializan los valores de las entradas de $W^{(l)}$ iguales, es equivalente a tener una sola neurona en esa capa, por lo que es necesario que estos valores sean diferentes. Para este ejemplo y con el fin de simplificar las operaciones de aprendizaje más adelante, vamos a asumir que la función de activación siempre será la función logística.\n",
    "\n",
    "Para efectos de un mejor aprendizaje, y asumiendo que la función de activación es la logistica, es importante que los valores iniciales de los pesos se encuentren en la zona donde casuan más variacion la función logística. Si asumimos que las entradas a cada neurona están normalizadas (esto es, entre 0 y 1), entonces los pesos deberían ser valores entre $(-\\sqrt{n_{l-1}}, \\sqrt{n_{l-1}})$ con el fin que la suma se encuentre en la región donde más cambios ocurren en la función logística. \n",
    "\n",
    "Vamos a generar y guardar esta información en un diccionario (junto con el resto de la información que requeriramos para tener una red neuronal completamente definida. Al principio los valores de normalización no cuentan ya que estos se deben inicializar al comienzo del aprendizaje.\n",
    "\n",
    "#### Ejercicio 1. Completa el código de la función de inicialización para los pesos de las matrices de pesos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso la prueba\n"
     ]
    }
   ],
   "source": [
    "def inicializa_red_neuronal(capas, neuronas_por_capa, tipo_salida):\n",
    "    \"\"\"\n",
    "    Inicializa una red neuronal como un diccionario de datos. \n",
    "    Se asume en este caso que la función de activación es la función logística\n",
    "    \n",
    "    @param capas: Un número entero con el número total de capas. Minimo 3 (una de entrada, una oculta, una de salida).\n",
    "    @param neuronas_por_capa: Una lista de enteros donde el primer elemento es el número de entradas\n",
    "                              y el último el número de salidas, mientras que los intermedios son\n",
    "                              el númerode neuronas en cada capa oculta.\n",
    "    @param tipo: Un string entre {'lineal', 'logistica', 'softmax'} con el tipo de función de salida de la red.\n",
    "    \n",
    "    @return: Un diccionario `rn` tal que\n",
    "             - rn['capas'] = capas\n",
    "             - rn['nxc'] = neuronas_por_capas\n",
    "             - rn['tipo'] = tipo\n",
    "             - rn['W'] = lista de matrices de parámetros\n",
    "             - rn['medias'] = lista de medias de cada atributo (se inicializan con puros 0)\n",
    "             - rn['std'] = lista de desviaciones estandard de cada atributo (se inicializan con puros 1)\n",
    "             \n",
    "    \"\"\"\n",
    "    if capas != len(neuronas_por_capa):\n",
    "        raise ValueError('El número de capas no corresponde con la lista de las neuronas por capa')\n",
    "    rn = {'capas': capas, 'nxc': neuronas_por_capa, 'tipo': tipo}\n",
    "    rn['medias'] = np.zeros(neuronas_por_capa[0])\n",
    "    rn['std'] = np.ones(neuronas_por_capa[0])    \n",
    "    rn['W'] = [inicializa_W(nla, nl) for (nla, nl) in zip(neuronas_por_capa[:-1], neuronas_por_capa[1:])]\n",
    "\n",
    "    return rn\n",
    "\n",
    "def inicializa_W( n_lm1, n_l):\n",
    "    \"\"\"\n",
    "    Inicializa una matriz de valores aleatorios W\n",
    "    \n",
    "    @param n_lm1: número de neuronas en la capa l-1 (entero)\n",
    "    @param n_l: número de neuronas en la capa l (entero)\n",
    "    \n",
    "    @return: Un ndarray de dimensión (n_l, n_lm1 + 1) donde las entradas son número aleatorios\n",
    "             entre -sqrt(n_lm1) y sqrt(n_lm1)\n",
    "             \n",
    "    \"\"\"\n",
    "    #------------------------------------------------------------------------\n",
    "    W = np.ndarray(shape=(n_l,n_lm1 + 1))\n",
    "    for i in range(0,n_l):\n",
    "        for j in range(0,n_lm1+1):\n",
    "            W[i][j] = np.random.uniform((-(np.sqrt(n_lm1))), (np.sqrt(n_lm1)))\n",
    "    return W\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "def test_inicializa_W():\n",
    "    #Vamos a hacer 1000 pruebas aleatorias que nos aseguremos que se cumpleen con las especificaciones\n",
    "    for _ in range(1000):\n",
    "        n0 = np.random.randint(1, 20)\n",
    "        n1 = np.random.randint(1, 20)\n",
    "        W = inicializa_W( n0, n1)\n",
    "        assert W.shape == (n1, n0 + 1)  # Las dimensiones son correctas\n",
    "        assert W.max() < np.sqrt(n0)    # La cota máxima se respeta\n",
    "        assert W.min() > -np.sqrt(n0)   # La cota mínima se respeta\n",
    "        assert np.abs(W).sum() > 0      # No estamos inicializando a 0\n",
    "    return \"Paso la prueba\"\n",
    "\n",
    "print(test_inicializa_W())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, si tenemos una red neuronal, la información contenida en el diccionario es toda la información específica que se necesita para la predicción, el aprendizaje, o el reaprendizaje de una red ya especificada. \n",
    "\n",
    "Como entrenar una red es algo lento y tedioso, y normalmente cuando hacemos un método de aprendizaje, lo que queremos es poder utilizarlo después para predecir un conjunto de datos no etiquetados previamente, es normal que guardemos en un archivo la información específica a la red neuronal, y despues la recuperemos en otra sesión, otro día, o en otra computadora para hacer la predicción.\n",
    "\n",
    "Una manera de guardar datos, funciones y objectos de Python en disco es utilizando el módulo ``pickle`` (o su versión compilada para mayor velocidad ``cPickle``). Este modulo permite guardar una serie de objetos de python en forma secuencial en un archivo binario, y luego recuperarlos. Notese que este métdo es diferente a ``np.load`` y ``np.savez``, ya que estos solo permiten guardar (y recuperar) una serie de ndarrays únicamente. \n",
    "\n",
    "Vamos entonces a hacer dos funciones muy simples ``guarda_objeto`` y ``carga_objeto``, que utilizaremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasa la prueba\n"
     ]
    }
   ],
   "source": [
    "def guarda_objeto(archivo, objeto):\n",
    "    \"\"\"\n",
    "    Guarda un objeto de python en el archivo \"archivo\". Si el archivo existe, sera reemplazado sin \n",
    "    preguntas, al puro estilo mafioso.\n",
    "    \n",
    "    @param archivo: string con el nombre de un archivo (aunque no exista)\n",
    "    @param objeto: Un objeto de python para ser guardado\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(archivo, 'wb') as arch:\n",
    "        cPickle.dump(objeto, arch, -1)\n",
    "        arch.close()\n",
    "        \n",
    "def carga_objeto(archivo):\n",
    "    \"\"\"\n",
    "    Carga el primer (y se asume que único) objeto contenido en el archivo 'archivo' que debe de ser tipo cPickle.\n",
    "    \n",
    "    @param archivo: string con el nombre de un archivo tipo pickle\n",
    "    @return: El primer objeto dentro del picke\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(archivo, 'rb') as arch:\n",
    "        objeto = cPickle.load(arch)\n",
    "        arch.close()\n",
    "        return objeto\n",
    "    \n",
    "def test_archivo():\n",
    "    \"\"\"\n",
    "    Prueba, para esto vamos a cargar o a leer (o ambas cosas) un objeto en un archivo\n",
    "    \n",
    "    Por favor, borrar el archivo cada vez que se pruebe, o para probar la lectura y la escritura\n",
    "    \n",
    "    \"\"\"\n",
    "    temp = [range(100), 'prueba', True]\n",
    "    guarda_objeto('prueba.pkl', temp)\n",
    "    temp =[10, 'no prueba', False]\n",
    "    otro = carga_objeto('prueba.pkl')\n",
    "    assert len(otro[0]) == 100\n",
    "    assert otro[1] == 'prueba'\n",
    "    assert otro[-1]\n",
    "    \n",
    "    return \"Pasa la prueba\"\n",
    "\n",
    "print (test_archivo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculando la salida de una red neuronal con *feedforward*\n",
    "\n",
    "\n",
    "Asumamos que tenemos una red neuronal ya inicializada, y que la vamos a utilizar para calcular el costo de una solución. Como vimos en clase, el costo de la solución depende del tipo de neuronas de salida (que son en realidad la etapa de clasificación). Así, para calcular el costo, es necesario calcular la salida de la red neuronal.\n",
    "\n",
    "Recordemos que el algoritmo para realizar la alimentación hacia adelante de una red neuronal el algoritmo es el siguiente:\n",
    "\n",
    "1. Normaliza los valores de entrada\n",
    "\n",
    "1. Inicializa $a^{(0)}$ asignandole los valores de las entradas normalizadas\n",
    "\n",
    "2. Por cada capa $l$ de 1 a $L-1$:\n",
    "\n",
    "    1. Se calcula el valor de $z^{(l)}$ como $$z^{(l)} = W^{(l)} a_e^{(l-1)},$$ donde $W^{(l)}$ es la \n",
    "       matriz de pesos de la capa $l-1$ a la capa $l$, y $a_e^{(l-1)}$ es $a^{(l-1)}$ extendida con un 1 al principio \n",
    "       el vector.\n",
    "       \n",
    "    2. Se calcula $a^{(l)}$ como $$a^{(l)} = g(z^{(l)}),$$ donde $g$ es la función de activación (en nuestro caso hemos \n",
    "       decidido utilizar la función logística, pero podríamos tener otras funciones de activación).\n",
    "\n",
    "3. Se calcula el valor de $z^{(L)}$ como $$z^{(L)} = W^{(L)} a_e^{(L-1)}.$$ \n",
    "\n",
    "4. Se calcula $a^{(L)}$ de acuerdo a la función de activación dependiendo del tipo de salida:\n",
    "\n",
    "    * Si `tipo = 'logistica'` entonces se utiliza la regresión logística (una sola neurona en la capa de salida).\n",
    "    * Si `tipo = 'lineal'` entonces $a^{(L)} = z^{(L)}$.\n",
    "    * Si `tipo = 'softmax'` entonces $a^{(L)} = softmax(z^{(L)}).$\n",
    "\n",
    "5. La salida de la red es $a^{(L)}$.\n",
    "\n",
    "Aqui hay que tomar en cuenta varias cosas: en primer lugar, la activación de todas las neuronas en todas las capas, y para todos los datos los necesitamos para realizar el algoritmo de *backpropagation*, por lo que se requiere guardarlos. Igualmente, no es eficiente calcular todos los pasos dato por dato, ya que eso lo haría muy, pero muy lento. Así que vamos a aporvechar que los datos vienen en forma de un *ndarray* de numpy y haremos todos los calculos en forma matricial tal como los vimos en clases. \n",
    "\n",
    "Sea $X$ la matriz de valores de entrada, entonces $A^{0} = X^T$ es una lista de vectores columna donde cada columna es $a^{(0)}$ para el objeto correspondiente. Así, los calculos se pueden realizar columna por columna, y simplemente\n",
    "$$ \n",
    "Z^{(l)} = W^{(l)}A_e^{(l-1)},\n",
    "$$\n",
    "donde $A_e^{(l-1)}$ es $A^{(l-1)}$ agregandole un 1 al inicio de cada vector (o lo que es lo mismo, agregandole un renglon de unos al inicio). Si procedemos de esta forma, entonces es importante recordar que al final $\\hat{Y} = (A^{(L)})^T$, ya que para la salida cada renglon es un dato diferente (de acuerdo a nuestra convención desde los otros algoritmos que hamos utilizado), mientras que internamente,para la red neuronal, cada columna proviene de un objeto diferente.\n",
    "\n",
    "#### Ejercicio 2. Completa el código de las funciones necesarias para realizar el algoritmo de *feedforward* para una red neuronal ya establecida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtiene_medias_desviaciones(x):\n",
    "    \"\"\"\n",
    "    Obtiene las medias y las desviaciones estandar atributo a atributo.\n",
    "    \n",
    "    @param x: un ndarray de dimensión (T, n) donde T es el númro de elementos y n el número de atributos\n",
    "    @return: medias, desviaciones donde ambos son ndarrays de dimensiones (n,) con las medias y las desviaciones \n",
    "             estandar respectivamente.\n",
    "    \n",
    "    \"\"\"\n",
    "    return x.mean(axis=0), x.std(axis=0)\n",
    "\n",
    "def normaliza(x, medias, desviaciones):\n",
    "    \"\"\"\n",
    "    Normaliza los datos x\n",
    "\n",
    "    @param x: un ndarray de dimensión (T, n) donde T es el númro de elementos y n el número de atributos\n",
    "    @param medias: ndarray de dimensiones (n,) con las medias con las que se normalizará\n",
    "    @param desviaciones: ndarray de dimensiones (n,) con las desviaciones con las que se normalizará\n",
    "    \n",
    "    @return: x_norm un ndarray de las mismas dimensiones de x pero normalizado\n",
    "    \n",
    "    \"\"\"\n",
    "    return (x - medias) / desviaciones\n",
    "\n",
    "\n",
    "def logistica(z):\n",
    "    \"\"\"\n",
    "    Calcula la función logística para cada elemento de z\n",
    "    \n",
    "    @param z: un ndarray\n",
    "    @return: un ndarray de las mismas dimensiones que z\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Calculo de la regresión softmax\n",
    "    \n",
    "    @param z: ndarray de dimensión (T, K) donde z[i, :] es el vector de aportes lineales de el objeto i    \n",
    "    @return: un ndarray de dimensión (T, K) donde cada columna es el calculo softmax de su respectivo vector de entrada.\n",
    "    \n",
    "    \"\"\"\n",
    "    y_hat = np.exp(z)\n",
    "    return y_hat / y_hat.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "\n",
    "def extendida(matriz):\n",
    "    \"\"\"\n",
    "    Agrega un renglon de unos a una matriz\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.r_[np.ones((1, matriz.shape[1])), matriz]\n",
    "\n",
    "def feedforward(X, red_neuronal):\n",
    "    \"\"\"\n",
    "    Calcula la salida estimada para los valores de `X` utilizando red_neuronal\n",
    "    \n",
    "    @param X: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    @param red_neuronal: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal``\n",
    "    \n",
    "    @return: `lista_A`, donde `Y_est` es un ndarray de shape (T, k) donde T es el número de objetos y k es \n",
    "             el número de salidas (neuronas de salida de la red neuronal).\n",
    "             \n",
    "    \"\"\"    \n",
    "    # Inicializar A^{(0)}\n",
    "    # ----------Agragar código aqui -----------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Despues vamos a hacer lo propio por cada capa hasta antes de la última\n",
    "    for wl in red_neuronal['W'][:-1]:         \n",
    "        # Calcula A_e^{l-1}, Z^{(l)}, A^{l} y agrega a lista_A.        \n",
    "        # (puede ser todo junto o por partes)\n",
    "        # ----------Agregar código aqui -----------------\n",
    "\n",
    "        \n",
    "        \n",
    "        lista_A.append(logistica(wl.dot(extendida(lista_A[-1]))))\n",
    "        \n",
    "    # Calcula A^{L} y agrega a lista_A de acuerdo al tipo de salida\n",
    "    # ----------Agregar código aqui -------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    return lista_A\n",
    "                                      \n",
    "    \n",
    "    \n",
    "def prueba_feedforward():\n",
    "    \"\"\"\n",
    "    Función para validar la función de fedforward \n",
    "    \n",
    "    Se inicializa una red de dos capas (más capa de entrada) y se \n",
    "    imponen los pesos. Esto se hizo a mano para las diferentes\n",
    "    entradas, así que se espera que la función de feedforward de\n",
    "    cosas similares\n",
    "\n",
    "    \"\"\"\n",
    "    # Inicializa red neuronal\n",
    "    rn = inicializa_red_neuronal(3, [2, 2, 1], 'lineal')\n",
    "    \n",
    "    # Modificamos pesos\n",
    "    rn['W'][0] = np.array([[0.5, -0.3, -0.7],\n",
    "                           [-1, 0.2, 0.3]])\n",
    "    rn['W'][1] = np.array([[0, 0.5, -0.5]])\n",
    "    \n",
    "    #Ponemos algunas entradas sencillas\n",
    "    x = np.array([[0,   0],\n",
    "                  [1,   1],\n",
    "                  [-1, -1]])\n",
    "    \n",
    "    # Y el valor de A calculado a mano\n",
    "    A1 = np.array([[0.6225, 0.3775, 0.8176],\n",
    "                   [0.2689, 0.3775, 0.1824]])\n",
    "    A2 = np.array([0.17676, 0, 0.3176])\n",
    "    \n",
    "    lista_A = feedforward(x, rn)\n",
    "    assert np.sum(np.abs(lista_A[1] - A1)) <= 0.001\n",
    "    assert np.sum(np.abs(lista_A[2] - A2)) <= 0.001\n",
    "    \n",
    "    #Ahora vamos a ver que pasa si cambiamos la salida\n",
    "    rn['tipo'] = 'logistica'\n",
    "    lista_A = feedforward(x, rn)\n",
    "    A2 = np.array([0.544075, 0.5, 0.5787])\n",
    "\n",
    "    assert np.sum(np.abs(lista_A[2] - A2)) <= 0.001\n",
    "    \n",
    "    return \"Paso la prueba\"\n",
    "\n",
    "print(prueba_feedforward())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calcula una función símple de pérdida de una red neuronal\n",
    "\n",
    "Con  la función de feedforward desarrollada, podemos hacer una función para calcular la función de pérdida de acuerdo a un conjunto de datos de entrada. La función de pérdida se puede expresar en relación a las salidas $Y$, las salidas estimadas por la red neuronal $\\hat{Y}$, y del tipo de salida. \n",
    "\n",
    "Las funciones de pérdida más comunes son:\n",
    "\n",
    "1. Para las redes con salidas lineales el MSE (Mean Square Error) dado por $$Loss(Y, \\hat{Y}) = \\frac{1}{N}(Y - \\hat{Y})^T(Y - \\hat{Y}).$$\n",
    "\n",
    "2. Para redes con salida logística la entropía en dos clases dada por $$Loss(Y, \\hat{Y}) = \\sum_{i=1}^N{-y^{(i)}\\log(\\hat{y}^{(i)}) - (1-y^{(i)})\\log(1-\\hat{y}^{(i)})}$$\n",
    "\n",
    "3. Para redes con salida *softmax* la entropia multiclase: $$Loss(Y, \\hat{Y}) = \\sum_{i=1}^N \\sum_{k=1}^K{1\\{y^{(i)} = k\\}\\log(\\hat{y}^{(i)})}$$\n",
    "\n",
    "#### Ejercicio 3: Completa el código de la función de perdida de acuerdo a las 3 salidas posibles de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perdida_red(Y, X, tipo, rn):\n",
    "        \"\"\"\n",
    "        Calcula la función de perdida de una red neuronal, de acuerdo al tipo de salida\n",
    "        y a un conjunto de datos conocidos expresados por Y y X\n",
    "\n",
    "        @param Y: un ndarray de dimension (T, K) con los valores de salida\n",
    "        @param X: un ndarray de dimension (T, N) con los valores de entrada\n",
    "        @param tipo: Un string el cual puede ser 'lineal', 'logistica'o 'softmax'\n",
    "\n",
    "        \"\"\"\n",
    "        lista_A = feedforward(X, rn)\n",
    "        Y_est = lista_A[-1].T\n",
    "        return perdida(Y, Y_est, tipo)\n",
    "\n",
    "\n",
    "\n",
    "def perdida(Y, Y_est, tipo):\n",
    "    \"\"\"\n",
    "    Calcula la función de perdida de una red neuronal, de acuerdo al tipo de salida \n",
    "    \n",
    "    @param Y: un ndarray de dimension (T, K) con los valores de salida\n",
    "    @param Y_est: un ndarray de dimension (T, K) con los valores de salida estimados\n",
    "    @param tipo: Un string que puede ser 'lineal', 'logistica'o 'softmax'\n",
    "    \n",
    "    \"\"\"\n",
    "    # ----------Agregar código aqui -------------------------------\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def prueba_perdida():\n",
    "    \"\"\"\n",
    "    La unidad de prueba de la función costo\n",
    "    \n",
    "    \"\"\"\n",
    "    Y = np.array([[.1, 2.2, 3.6, 0]]).T\n",
    "    Y_est = np.array([[.3, 2.0, 3.8, -0.2]]).T\n",
    "    assert abs(perdida(Y, Y_est, 'lineal') - 0.02) < 1e-8\n",
    "    \n",
    "    Y = np.array([[1, 0, 1, 0]]).T\n",
    "    Y_est = np.array([[.9, 0.1, 0.8, 0.2]]).T\n",
    "    assert abs(perdida(Y, Y_est, 'logistica') - 0.164252033486018) < 1e-8\n",
    "    \n",
    "    Y = np.array([[1, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]).T\n",
    "    Y_est = np.array([[0.8, 0.1, 0.1], \n",
    "                      [0.15, 0.8, 0.05],\n",
    "                      [0.9, 0.05, 0.05],\n",
    "                      [0.021, 0.079, 0.9]])\n",
    "    assert abs(perdida(Y, Y_est, 'softmax') - 0.164252033486018) < 1e-8\n",
    "    \n",
    "    return 'paso la prueba'\n",
    "\n",
    "print prueba_costo()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
