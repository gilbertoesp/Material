{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imagenes/letragrama-rgb-150.jpg\" width=\"380\" align=\"left\"><img src=\"imagenes/logoLCCazul.jpg\" width=\"170\" align=\"right\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Aprendizaje en redes neuronales: El algoritmo de *backpropagation*\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 18 de septiembre de 2017.\n",
    "\n",
    "En esta libreta vamos a practicar y revisar el principal algoritmo de aprendizaje de los pesos de una  red neuronal hacia adelante. Todos los algoritmos más sofisticados son versiones y modificaciones al algoritmo de *backpropagation* o *B-prop*. Para aplicaciones reales vamos autilizar herramientas poderosas como [Tensorflow](https://www.tensorflow.org) o [Theano](http://www.deeplearning.net/software/theano/), pero es importante conocer y saber desarrollar el algoritmo básico para entender como funciona el aprendizaje en general en redes neuronales.\n",
    "\n",
    "Vamos a asumir que todos han realizado la libreta anterior donde desarrollaron una estructura básica de una red neuronal, así como el algoritmo de reconocimiento, o *feedforward*. Para una explicacion de la estructura, es necesario que se remitan a la libreta anterior. Sin embarog resumirenos los pasos principales:\n",
    "\n",
    "\n",
    "\n",
    "Empecemos por inicializar los modulos que vamos a requerir, así como las funciones desarrolladas en otras libretas para el calculo en feedforward de redes neuronales artificiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from feedforward import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones ya programadas las dejamos en un archivo adjunto que se llama `feedforward.py` el cual contiene las operaciones que se desarrollaron anteriormente y que agregarlas en esta libreta metería más ruido de lo que ayudaría a la comprensión. El archivo se encuentra adjunto y eres libre de consultarlo y revisarlo. En este caso nos comportamos muy poco formales y cargamos todas las funciones en el espacio de trabajo principal. Esto es porque vamos a hacer de cuenta que las acabamos de programar.\n",
    "\n",
    "En las libretas de *Jupyter*, cuando quieres consultar la documentación de una función utilizas la combinación de teclas *shift*-*tab*. Si lo haces una vez aparece únicamente el docstring en una ventana emergente. Si utilizas *shift*-*tab* dos veces, aparece la documentación extendida (si hay), y con 3 veces la documentación completa pero en ventana emergente. Con 4 veces la documentación completa aparece en un *frame* abajo y se mantiene mientras tecleas, hasta que se cierre. Este ultimo es especialmente útil para revisar documentación de una función mientras decide uno como utilizarla.\n",
    "\n",
    "#### Ejercicio 1: Revisa la documentación de las funciones que se listan a continuación, ya que las vamos a estar utilizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Revisa la documentación de cada uno de las soguientes funciones\n",
    "\n",
    "inicializa_red_neuronal\n",
    "logistica\n",
    "extendida\n",
    "feedforward\n",
    "perdida\n",
    "perdida_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Como puedes ver las funciones anteriores (que puedes consultar en el archivo `feedforard.py`), nos permiten definir una red neuronal, estimar la salida para un conjunto de datos, así como estimar la función de pérdda con el mismo, u otro conjunto de datos.\n",
    "\n",
    "## El algortmo de *backpropagation*\n",
    "\n",
    "La derivada parcial de la función de pérdida respecto a cada uno de los pesos (por lo tanto el gradiente de la función de pérdida respecto al vector de todos los pesos) se obtiene mediante el algoritmo de backpropagarion. Una vez que se conoce esto, entonces es posible utilizar el resultado para optimizar los pesos mediente el método de descenso de gradiente o una variente similar.\n",
    "\n",
    "El algoritmo de *backpropagation* (de hecho en los últimos años se conoce como *b-prop*, y solo los rucos decimos *backpropagation*) es el siguiente:\n",
    "\n",
    "1. Calcular el vector $\\delta^{(L)}$ para todos los datos a partir de $A^{L}$, $Y$ y el tipo de salida. Si utilizamos la función de pérdida correcta para cada tipo de salida, entonces esto se calcula en forma muy sencilla, tal como vimos en clases de la forma $$\\delta^{(L)} = Y^T - A^{(L)}.$$\n",
    "\n",
    "2. Para $l$ de $L-1$ hasta 1 con decrementos de $-1$:\n",
    "    1. Calcular $\\delta^{(l)}$ a partir de $\\delta^{(l+1)}$, $W^{(l+1)}$ y $A^{(l)}$ como\n",
    "       $$\n",
    "       \\delta^{(l)} = A^{(l)} \\star (\\vec{1} - A^{(l)}) \\star (r(W^{(l+1)})^T \\delta^{(l+1)}), \n",
    "       $$\n",
    "       donde $\\star$ es la multiplicación elemento a elemento de dos matrices, $\\vec(1)$ es una matriz con 1 en todos sus elementos y $r(\\cdot)$ es una función que elimina la primer columna de una matriz.\n",
    "    2. Calcular la derivada en cada uno de los pesos como\n",
    "       $$\n",
    "       \\frac{\\partial J(W)}{w_{ij}^{(l)}} = \\frac{1}{card(CA)}\\sum_{\\forall (x,y) \\in CA} a_j^{(l-1)} \\delta_i^{(l)},\n",
    "       $$\n",
    "       para lo cual vimos como realizarlo en forma matricial y eficiente en clases.\n",
    "       \n",
    "El desarrollo del algoritmo y los pasos en forma matricial se vieron con detalle en clase, por lo que aqui solo se da un pequeñño bosquejo esperando que se programe correctamente.\n",
    "\n",
    "#### Ejercicio 1: Completa el código de la función de backpropagation (20 puntos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La función `r`, cambiada como `r_fun` para evitar confuciones \n",
    "def r_fun(matriz):\n",
    "    return matriz[:, 1:]\n",
    "\n",
    "def backpropagation(Y, A, rn):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente de los pesos de una red neuronal\n",
    "    \n",
    "    Parametros\n",
    "    -----------\n",
    "    Y: ndarray de shape (N, k) donde N es el número de ejemplos y k el número de salidas\n",
    "    \n",
    "    A: Una lista de matrices de activaciones por capas, obtenidas por la función `feedforward`,\n",
    "       en donde A[l] es un ndarray de shape (nl, N), donde N es el número de ejemplos evaluados \n",
    "       y nl es el número de neuronas de la capa l de rn.\n",
    "\n",
    "    rn: Estructura de datos de una red neuronal inicializada con la función \n",
    "        `inicializa_red_neuronal`\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    Una lista de gradientes [None, gW1, gW2, ..., gWL], donde cada gWl es un ndarray tal que \n",
    "    rn['W'][l].shape == gWl.shape\n",
    "             \n",
    "    \"\"\"    \n",
    "    # Numero de ejemplos\n",
    "    N = Y.shape[0]\n",
    "    \n",
    "    # Inicializa la lista de gradientes en 0\n",
    "    gradientes = [None] + [np.zeros_like(Wl) for Wl in rn['W'][1:]]\n",
    "\n",
    "    # Calcula Delta para la capa de salida\n",
    "    delta = Y.T - A[-1]\n",
    "    \n",
    "    #Calcula el gradiente de los pesos de la última capa\n",
    "    gradientes[-1] = -delta.dot(extendida(A[-2]).T) / N\n",
    "    \n",
    "    # Despues vamos a hacer lo propio por cada capa hasta antes de la última\n",
    "    for l in range(rn['capas'] - 2, 0, -1):         \n",
    "        \n",
    "        # Calcula la delta para la capa anterior.        \n",
    "        # ----------Agregar código aqui -----------------\n",
    "\n",
    "        \n",
    "        \n",
    "        # Calcula el gradiente para los pesos de la capa anterior.        \n",
    "        # ----------Agregar código aqui -----------------\n",
    "\n",
    "        \n",
    "        \n",
    "    return gradientes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisar y corregir código de aprendizaje máquina\n",
    "\n",
    "El problema con este tipo de algoritmos es que, a la hora de codificarlos, es muy típico que se tengan errores, tanto de concepto como de codificación. Sin embargo, como estos algortimos se utilizan para aprender, y al utilizar un conjunto inicial de pesos aleatorio diferente, los resultados no son verificables. Es muy común tener un error muy tonto y pasar muchas horas (o días) intentando corregirlo.\n",
    "\n",
    "Por esta razón, siempre hay que hacer métodos que nos permitan chacar que el algortimo parece funcionar correctamente. Para eso, vamos a programar una forma alternativa de calcular una aproximación del gradiente en forma numérica. Este algortimo de fuerza bruta es altamente ineficiente y no puede ser utilizada dentro de un método de optimización pero nos sirve para revisar si existen errores que podríamos haber ingresado en nuestro algoritmo.\n",
    "\n",
    "El método se basa en el calculo numérico de una derivada parcial como:\n",
    "\n",
    "$$\n",
    "  \\left.\\frac{\\partial f(x)}{\\partial x}\\right|_{x = x_0} \\approx \\frac{f(x_0 + \\epsilon) - f(x_0 - \\epsilon)}{2 \\epsilon}.  \n",
    "$$\n",
    "\n",
    "Entonces, si queremos estimar el gradiente de la función de pérdida respecto a los pesos, hay que calcular esta razón por cada uno de los pesos, por cada una de las capas. Esto no es nada eficiente y mucho menos elegante (como el *b-prop*) pero es un método de validación.\n",
    "\n",
    "#### Ejercicio: Completa el código de la función de gradiente numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradiente_numerico(X, Y, rn, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente numérico para efectos de prueba del algoritmo de backpropagation.\n",
    "    Este algortimo se realiza expresamente de manera ineficiente pero clara, ya que su\n",
    "    funcionamiento debe de ser lo más claro posible.\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    X: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    Y: ndarray de shape (T, k) donde k es el número de salidas\n",
    "    rn: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal``\n",
    "    epsilon: Un número flotante positivo, típicamente muy pequeño\n",
    " \n",
    "    Devuelve\n",
    "    --------\n",
    "    Una lista de gradientes de red_neuronal['W'] con la misma estructura y dimensión\n",
    "    \n",
    "    \"\"\"\n",
    "    # inicializa los gradientes a cero\n",
    "    gradientes = [None] + [np.ones_like(Wl) for Wl in rn['W'][1:]]\n",
    "    \n",
    "    for l in range(1,rn['capas']):                #  Por cada capa l\n",
    "        for i in range(rn['W'][l].shape[0]):          #  Por cada renglon i\n",
    "            for j in range(rn['W'][l].shape[1]):      #  Por cada columna j\n",
    "                # -------------------------------------------\n",
    "                # Insertar código aquí\n",
    "                # -------------------------------------------\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                # --------------------------------------------\n",
    "    return gradientes           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer una función de prueba utilizando un conjunto de datos reales (o un subconjunto de estos), y lo vamos a hacer para muchas posibles reinicializaciones de pesos. Este código va a servir para corregir ambos algoritmos de calculo de gradiente.  Para esto vamos a utilizar una base de datos ya conocida, la de dígitos, utilizada en la libreta de regresión softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Datos a utilizar para la prueba\n",
    "# para no tener que estarlos cargando de nuevo cada vez\n",
    "\n",
    "# Vamos a utilizar un subconkunto de datos y de atributos\n",
    "# para que pueda funcionar el gradiente numérico en un tiempo \n",
    "# aceptable\n",
    "\n",
    "\n",
    "data = np.load(\"datos/digitos.npz\")\n",
    "x_prueba = data['X_entrena'][:100,:10]  # Solo 100 datos y 10 parámetros\n",
    "y_prueba = data['T_entrena'][:100,:]    # Todas las clases de los primeros 100 datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Favor de no seguir más allá en la programación de la red neuronal hasta estar seguro que esto funcione correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_gradiente(X, Y):\n",
    "    \"\"\"\n",
    "    Unidad de prueba de backpropagation\n",
    "    \n",
    "    \"\"\"\n",
    "    n0 = X.shape[1]\n",
    "    nL = Y.shape[1]\n",
    "    \n",
    "    for ocultas in [2, 3]: \n",
    "        for neuronas in [2, 3]:\n",
    "            rn = inicializa_red_neuronal(ocultas + 2, \n",
    "                                         [n0] + (ocultas * [neuronas * n0]) + [nL],\n",
    "                                         'lineal')\n",
    "\n",
    "            for tipo in ['lineal', 'logistica', 'softmax']:\n",
    "                rn['tipo'] = tipo\n",
    "                A = feedforward(X, rn)\n",
    "                gradientes = backpropagation(Y, A, rn)\n",
    "                gradientes_numericos = gradiente_numerico(X, Y, rn, 1e-3)\n",
    "                diferencias = [np.abs(g - gn).max() \n",
    "                               for (g, gn) in zip(gradientes[1:], \n",
    "                                                  gradientes_numericos[1:])]\n",
    "                print(\"Tipo: {}, Neuronas: {}\".format(rn['tipo'], rn['nxc']))\n",
    "                print(\"Máxima diferencia entre métodos de gradiente: {}\".format(max(diferencias)))\n",
    "                if max(diferencias) > 1e-3:\n",
    "                    print('Gradiente: \\n{}'.format(gradientes[-1][0,:]))\n",
    "                    print('Gradiente Numérico: \\n{}'.format(gradientes_numericos[-1][0,:]))\n",
    "\n",
    "                assert max(diferencias) < 1e-3\n",
    "    return \"Paso la prueba\"\n",
    "\n",
    "print(prueba_gradiente(x_prueba, y_prueba))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El algoritmo de descenso de gradiente \n",
    "\n",
    "Ya que estamos seguros que el método de *b-prop* funciona correcatamente, entonces ya podemos poner todo junto muy fácilmente para realizar aprendizaje. Para esto vamos a utilizar el método de descenso de gradiente, el cual, en su forma más simple por lotes no es más que repetir por un número de veces (*epochs*) el calculo del gradiente y modificar los pesos de la red en dirección contraria a estos. \n",
    "\n",
    "En forma de algoritmo esto sería realizar los siguientes pasos:\n",
    "\n",
    "1. De 1 a `max_epochs`:\n",
    "    1. Calcular las activaciones `A` con el algoritmo de *feedforward*\n",
    "    2. Calcular los gradientes $\\nabla W^{(l)}$ para toda capa $l$, con el algoritmo de *backpropagation*.\n",
    "    3. Actualizar los pesos como\n",
    "       $$\n",
    "       W^{(l)} \\leftarrow W^{(l)} - \\alpha \\nabla W^{(l)} \n",
    "       $$\n",
    "       \n",
    "Como vemos, en este algoritmo solo hay que especificar dos variables, la tasa de aprendizaje $\\alpha$ y el número máximo de *epochs*. Este algoritmo solo lo ponemos como una primera aproximación, pero es un método de base del cual partiremos para realizar mejores algoritmos de aprendizaje.\n",
    "\n",
    "#### Ejercicio: Completa el código de la función de descenso de gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_grad_simple(rn, X, Y, alfa=0.2, max_epochs=1000, normaliza=False):\n",
    "    \"\"\"\n",
    "    Entrena una red neuronal utilizando el método de descenso de gradiente simple\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    rn: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal``\n",
    "    X: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    Y: ndarray de shape (T, k) donde k es el número de salidas\n",
    "    alfa: La tasa de aprendizaje, un número positivo típicamente menor que 1\n",
    "    max_epochs: Número de epocas o iteraciones del algoritmo. Un entero positivo.\n",
    "    normaliza: Un booleano para usar X y Y para normalizar o no los datos antes de entrar a la rn.\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    None, la función utiliza el hecho que rn es un objeto mutabe y modifica rn['W'] directamente.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Encuentra medias y desviaciones si aplica\n",
    "    if normaliza:\n",
    "        rn['medias'], rn['std'] = obtiene_medias_desviaciones(X)\n",
    "        \n",
    "    # Aprendizaje\n",
    "    for _ in range(max_epochs):\n",
    "        # Calcula las activaciones\n",
    "        # -------------------------------------------\n",
    "        # Insertar código aquí\n",
    "        # -------------------------------------------\n",
    "\n",
    "        \n",
    "        # Calcula lss gradientes\n",
    "        # -------------------------------------------\n",
    "        # Insertar código aquí\n",
    "        # -------------------------------------------\n",
    "\n",
    "        \n",
    "        \n",
    "        # Actualiza los pesos\n",
    "        # -------------------------------------------\n",
    "        # Insertar código aquí\n",
    "        # -------------------------------------------\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora hay que probar que el algoritmo funciona para un problema sencillo que nos permita encontrar problemas en su codificación. Para esto vamos a generar un conjunto de datos de entrenamiento, los cuales los podremos visualizar fácilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Biblioteca de graficación y configuracion de figuras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "#  Inicialización de datos\n",
    "np.random.seed(0) # Aseguramos que siempre pasa lo mismo\n",
    "N = 100 # Ejemplos por clase\n",
    "D = 2 # Atributos\n",
    "K = 3 # Clases\n",
    "X = np.zeros((N * K, D))\n",
    "Y = np.zeros((N * K, K), dtype='uint8')\n",
    "y = np.zeros((N * K, 1), dtype='uint8')\n",
    "\n",
    "# Genera datos en espiral\n",
    "for clase in range(K):\n",
    "  ix = list(range(N*clase, N*(clase+1)))  # Indices para cada clase\n",
    "  r = np.linspace(0.0, 1, N) \n",
    "  t = np.linspace(clase * 4, (clase + 1) * 4, N) + np.random.randn(N) * 0.2 \n",
    "  X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n",
    "  Y[ix, clase] = 1\n",
    "  y[ix] = clase\n",
    "\n",
    "\n",
    "#  Grafica datos\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([-1,1])\n",
    "plt.title(\"Datos sintéticos para probar el algoritmo de descenso de gradiente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, los datos generados tienen más de dos clases y no son linealmente separables. De lo que se infiere que el algoritmo más adaptado a este problema es una red neuronal con salida *softmax* y con al menos una capa oculta (o dos). \n",
    "\n",
    "En un primer paso, yo voy a hacer una red tipo softmax sin capas ocultas, la cual tendría que entrenarse fácilmente y debería ser fácil de visualizar si los resultados son correctos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn = inicializa_red_neuronal(capas=2, neuronas_por_capa=[2, 3], tipo_salida='softmax')\n",
    "\n",
    "desc_grad_simple(rn, X, Y, alfa=0.2, max_epochs=1000)\n",
    "\n",
    "Y_est = feedforward(X, rn)[-1].T\n",
    "y_est = np.argmax(Y_est, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a visualizar los resultados a ver si tienen sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera un grido en todo el espacio con datos a clasificar\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "X_mesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Calcula la predicción de clase para estos datos\n",
    "Y_mesh = feedforward(X_mesh, rn)[-1].T\n",
    "y_mesh = np.argmax(Y_mesh, axis=1)\n",
    "\n",
    "# Ajusta las salidas en forma de matriz para graficar el contorno\n",
    "y_mesh = y_mesh.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, y_mesh, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral, edgecolors='black')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Separación lineal de los datos sintéticos con una red neuronal sin capas ocultas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y si el programa funciona correctamente, el resultado debería ser una imagen como esta:\n",
    "\n",
    "![Separación lineal de los datos sintéticos con una red neuronal sin capas ocultas](imagenes/espiral_linear.png)\n",
    "\n",
    "Si este es el caso, entonces ya estamos listos para probar el algoritmo de descenso de gradiente para una red neuronal con capas ocultas.\n",
    "\n",
    "#### Ejercicio: Realiza el aprendizaje con una red neuronal con capas ocultas, de forma que todos los datos sean correctamente clasificados y grafíca el resultado de manera similar a como se hizo para la red neuronal sin capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza el aprendizaje con una red neuronal con capas ocultas, \n",
    "# de forma que todos los datos sean correctamente clasificados \n",
    "# y grafíca el resultado de manera similar a como se hizo para \n",
    "# la red neuronal sin capas ocultas.\n",
    "\n",
    "# El resultado de esta celfa debe de ser una figura similar a la \n",
    "# anterior pero que se pueda ver la partición no lineal del espacio\n",
    "\n",
    "# -------------------------------------------\n",
    "# Insertar código aquí\n",
    "# -------------------------------------------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
