{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales hacia adelante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta vamos a realizar las funciones necesarias para entrenar y predecir utilizando una red neuronal hacia adelante multicapa, con función de activación logística en *todas* las neuronas de las capas ocultas. Esta libreta no pretende sustituir a las explicaciones en clase o a unas notas sobre redes neuronales. Aqui se asume que ustedes ya tienen una idea general de las redes neuronales, que comprenden el algoritmo de *backpropagation* tal como lo desarrollamos en clase. En esta libreta *solamente* nos vamos a centrar en los aspectos de implementación.\n",
    "\n",
    "Para empezar esta libreta, necesitaremos algunas de las funciones que ya programamos en libretas pasadas, las cuales adjuntamos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "from scipy.optimize import minimize\n",
    "from IPython.display import Image \n",
    "\n",
    "\n",
    "def obtiene_medias_desviaciones(x):\n",
    "    \"\"\"\n",
    "    Obtiene las medias y las desviaciones estandar atributo a atributo.\n",
    "    \n",
    "    @param x: un ndarray de dimensión (T, n) donde T es el númro de elementos y n el número de atributos\n",
    "    @return: medias, desviaciones donde ambos son ndarrays de dimensiones (n,) con las medias y las desviaciones \n",
    "             estandar respectivamente.\n",
    "    \n",
    "    \"\"\"\n",
    "    return x.mean(axis=0), x.std(axis=0)\n",
    "\n",
    "def normaliza(x, medias, desviaciones):\n",
    "    \"\"\"\n",
    "    Normaliza los datos x\n",
    "\n",
    "    @param x: un ndarray de dimensión (T, n) donde T es el númro de elementos y n el número de atributos\n",
    "    @param medias: ndarray de dimensiones (n,) con las medias con las que se normalizará\n",
    "    @param desviaciones: ndarray de dimensiones (n,) con las desviaciones con las que se normalizará\n",
    "    \n",
    "    @return: x_norm un ndarray de las mismas dimensiones de x pero normalizado\n",
    "    \n",
    "    \"\"\"\n",
    "    return (x - medias) / desviaciones\n",
    "\n",
    "\n",
    "def logistica(z):\n",
    "    \"\"\"\n",
    "    Calcula la función logística para cada elemento de z\n",
    "    \n",
    "    @param z: un ndarray\n",
    "    @return: un ndarray de las mismas dimensiones que z\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Calculo de la regresión softmax\n",
    "    \n",
    "    @param z: ndarray de dimensión (T, K) donde z[i, :] es el vector de aportes lineales de el objeto i    \n",
    "    @return: un ndarray de dimensión (T, K) donde cada columna es el calculo softmax de su respectivo vector de entrada.\n",
    "    \n",
    "    \"\"\"\n",
    "    y_hat = np.exp(z)\n",
    "    return y_hat / y_hat.sum(axis=1).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Especificando una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, para poder hacer una red neuronal, tenemos que determinar cierta información. Por el momento, y para efecto de la libreta, vamos a mantenernos en un esquema estructurado/funcional, y más adelante vamos a generalizar la mayoría de los métodos utilizados en forma de objetos.\n",
    "\n",
    "La información importante que debemos de tener en cuenta cuando hacemos un sistema de redes neuronales es:\n",
    "\n",
    "- Cuantas capas de neuronas tiene la red neuronal, $L$.\n",
    "- Cuantas neuronas va a tener cada capa $[n_0, n_1, \\ldots, n_L]$, donde $n_0$ es el número de entradas y $n_L$ el número de salidas.\n",
    "- Cual es el tipo de salida de mi red neuronal (lineal, logística o softmax)\n",
    "- Los valores con los que se normalizan los datos de entrada a la red neuronal (para el aprendizaje en una red neuronal es muy importante que los valores de entrada estén normalizados).\n",
    "\n",
    "Una vez que se establecen estos valores, es necesario generar una lista de matrices $[W^{(1)}, \\ldots, W^{(L)}]$ donde $W^{(l)}$ es una matriz de dimensiones $(n_l, n_{l-1} + 1)$ de parámetros o pesos. Como vimos en clase, si se inicializan los valores de las entradas de $W^{(l)}$ iguales, es equivalente a tener una sola neurona en esa capa, por lo que es necesario que estos valores sean diferentes. \n",
    "\n",
    "En clase vimos que, para efectos de un mejor aprendizaje, es importante que los valores de entrada se encientren en la zona donde casua más variacion la función logística. Para esto, se espera que en general la suma de los pesos multiplicados por las entradas correspondientes a la capa se encuentren en el rango de $(-1, 1)$. Si asumimos que las entradas a cada neurona están normalizadas (esto es, entre 0 y 1), entonces los pesos deberían ser valores entre $(-\\sqrt{n_{l-1}}, \\sqrt{n_{l-1}})$ con el fin que la suma se encuentre en la región donde más cambios ocurren en la función logística. \n",
    "\n",
    "Vamos a generar y guardar esta información en un diccionario (junto con el resto de la información que requeriramos para tener una red neuronal completamente definida. Al principio los valores de normalización no cuentan ya que estos se deben inicializar al comienzo del aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1. Completa el código de la función de inicialización para los pesos de las matrices de pesos (10 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializa_red_neuronal(capas, neuronas_por_capa, tipo):\n",
    "    \"\"\"\n",
    "    Inicializa una red neuronal como un diccionario de datos\n",
    "    \n",
    "    @param capas: Un número entero con el número total de capas. Minimo 3 (una de entrada, una oculta, una de salida).\n",
    "    @param neuronas_por_capa: Una lista de enteros donde el primer elemento es el número de entradas\n",
    "                              y el último el número de salidas, mientras que los intermedios son\n",
    "                              el númerode neuronas en cada capa oculta.\n",
    "    @param tipo: Un string entre {'lineal', 'logistica', 'softmax'} con el tipo de salida de la red.\n",
    "    \n",
    "    @return: Un diccionario `rn` tal que\n",
    "             - rn['capas'] = capas\n",
    "             - rn['nxc'] = neuronas_por_capas\n",
    "             - rn['tipo'] = tipo\n",
    "             - rn['W'] = lista de matrices de parámetros\n",
    "             - rn['medias'] = lista de medias de cada atributo (se inicializan com puros 0)\n",
    "             - rn['std'] = lista de desviaciones estandard de cada atributo (se inicializan con puros 1)\n",
    "             \n",
    "    \"\"\"\n",
    "    if capas != len(neuronas_por_capa):\n",
    "        raise ValueError('El número de capas no corresponde con la lista de las neuronas por capa')\n",
    "    rn = {'capas': capas, 'nxc': neuronas_por_capa, 'tipo': tipo}\n",
    "    rn['medias'] = np.zeros(neuronas_por_capa[0])\n",
    "    rn['std'] = np.ones(neuronas_por_capa[0])\n",
    "    \n",
    "    rn['W'] = []\n",
    "    for (nla, nl) in zip(neuronas_por_capa[:-1], neuronas_por_capa[1:]):\n",
    "        rn['W'].append(inicializa_W(nla, nl))\n",
    "\n",
    "    return rn\n",
    "\n",
    "def inicializa_W( n_lm1, n_l):\n",
    "    \"\"\"\n",
    "    Inicializa una matriz de valores aleatorios W\n",
    "    \n",
    "    @param n_lm1: número de neuronas en la capa l-1 (entero)\n",
    "    @param n_l: número de neuronas en la capa l (entero)\n",
    "    \n",
    "    @return: Un ndarray de dimensión (n_l, n_lm1 + 1) donde las entradas son número aleatorios\n",
    "             entre -sqrt(n_lm1) y sqrt(n_lm1)\n",
    "             \n",
    "    \"\"\"\n",
    "    #------------------------------------------------------------------------\n",
    "    # Agregua aqui tu código\n",
    "    \n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "def test_inicializa_W():\n",
    "    #Vamos a hacer 1000 pruebas aleatorias que nos aseguremos que se cumpleen con las especificaciones\n",
    "    for _ in range(1000):\n",
    "        n0 = np.random.randint(1, 20)\n",
    "        n1 = np.random.randint(1, 20)\n",
    "        W = inicializa_W( n0, n1)\n",
    "        assert W.shape == (n1, n0 + 1)  # Las dimensiones son correctas\n",
    "        assert W.max() < np.sqrt(n0)    # La cota máxima se respeta\n",
    "        assert W.min() > -np.sqrt(n0)   # La cota mínima se respeta\n",
    "        assert np.abs(W).sum() > 0      # No estamos inicializando a 0\n",
    "    return \"Paso la prueba\"\n",
    "\n",
    "print test_inicializa_W()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, si tenemos una red neuronal, la información contenida en el diccionario es toda la información específica que se necesita para la predicción, el aprendizaje, o el reaprendizaje de una red ya especificada. \n",
    "\n",
    "Como entrenar una red es algo lento y tedioso, y normalmente cuando hacemos un método de aprendizaje, lo que queremos es poder utilizarlo después para predecir un conjunto de datos no etiquetados previamente, es normal que guardemos en un archivo la información específica a la red neuronal, y despues la recuperemos en otra sesión, otro día, o en otra computadora para hacer la predicción.\n",
    "\n",
    "Una manera de guardar datos, funciones y objectos de Python en disco es utilizando el módulo ``pickle`` (o su versión compilada para mayor velocidad ``cPickle``). Este modulo permite guardar una serie de objetos de python en forma secuencial en un archivo binario, y luego recuperarlos. Notese que este métdo es diferente a ``np.load`` y ``np.savez``, ya que estos solo permiten guardar (y recuperar) una serie de ndarrays únicamente. \n",
    "\n",
    "Vamos entonces a hacer dos funciones muy simples ``guarda_objeto`` y ``carga_objeto``, que utilizaremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guarda_objeto(archivo, objeto):\n",
    "    \"\"\"\n",
    "    Guarda un objeto de python en el archivo \"archivo\". Si el archivo existe, sera reemplazado sin \n",
    "    preguntas, al puro estilo mafioso.\n",
    "    \n",
    "    @param archivo: string con el nombre de un archivo (aunque no exista)\n",
    "    @param objeto: Un objeto de python para ser guardado\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(archivo, 'wb') as arch:\n",
    "        cPickle.dump(objeto, arch, -1)\n",
    "        arch.close()\n",
    "        \n",
    "def carga_objeto(archivo):\n",
    "    \"\"\"\n",
    "    Carga el primer (y se asume que único) objeto contenido en el archivo 'archivo' que debe de ser tipo cPickle.\n",
    "    \n",
    "    @param archivo: string con el nombre de un archivo tipo pickle\n",
    "    @return: El primer objeto dentro del picke\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(archivo, 'rb') as arch:\n",
    "        objeto = cPickle.load(arch)\n",
    "        arch.close()\n",
    "        return objeto\n",
    "    \n",
    "def test_archivo():\n",
    "    \"\"\"\n",
    "    Prueba, para esto vamos a cargar o a leer (o ambas cosas) un objeto en un archivo\n",
    "    \n",
    "    Por favor, borrar el archivo cada vez que se pruebe, o para probar la lectura y la escritura\n",
    "    \n",
    "    \"\"\"\n",
    "    temp = [range(100), 'prueba', True]\n",
    "    guarda_objeto('prueba.pkl', temp)\n",
    "    temp =[10, 'no prueba', False]\n",
    "    otro = carga_objeto('prueba.pkl')\n",
    "    assert len(otro[0]) == 100\n",
    "    assert otro[1] == 'prueba'\n",
    "    assert otro[-1]\n",
    "    \n",
    "    return \"Pasa la prueba\"\n",
    "\n",
    "print test_archivo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculando el costo (y por lo tanto feed-forward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asumamos que tenemos una red neuronal ya inicializada, y que la vamos a utilizar para calcular el costo de una solución. Como vimos en clase, el costo de la solución depende del tipo de neuronas de salida (que son en realidad la etapa de clasificación). Así, para calcular el costo, es necesario calcular la salida de la red neuronal.\n",
    "\n",
    "Recordemos que el algoritmo para realizar la alimentación hacia adelante de una red neuronal el algoritmo es el siguiente:\n",
    "\n",
    "1. Inicializa $a^{(0)}$ asignandole los valores de las entradas\n",
    "\n",
    "2. Por cada capa $l$ de 1 a $L-1$:\n",
    "\n",
    "    1. Se calcula el valor de $z^{(l)}$ como $$z^{(l)} = W^{(l)} a_e^{(l-1)},$$ donde $W^{(l)}$ es la \n",
    "       matriz de pesos de la capa $l-1$ a la capa $l$, y $a_e{(l-1)}$ es $a^{(l-1)}$ extendida con un 1 al principio \n",
    "       el vector.\n",
    "       \n",
    "    2. Se calcula $a^{(l)}$ como $$a^{(l)} = g(z^{(l)}),$$ donde $g$ es la función de activación (en nuestro caso hemos \n",
    "       decidido utilizar la función logística, pero podríamos tener otras funciones de activación).\n",
    "\n",
    "3. Se calcula el valor de $z^{(L)}$ como $$z^{(L)} = W^{(L)} a_e^{(L-1)}.$$ \n",
    "\n",
    "4. Se calcula $a^{(L)}$ de acuerdo a la función de activación dependiendo del tipo de salida:\n",
    "\n",
    "    * Si `tipo = 'logistica'` entonces se utiliza la regresión logística (una sola neurona en la capa de salida).\n",
    "    * Si `tipo = 'lineal'` entonces $a^{(L)} = z^{(L)}$.\n",
    "    * Si `tipo = 'softmax'` entonces $a^{(L)} = softmax(z^{(L)}).$\n",
    "\n",
    "5. La salida de la red es $a^{(L)}$.\n",
    "\n",
    "Aqui hay que tomar en cuenta varias cosas: en primer lugar, la activación de todas las neuronas en todas las capas, y para todos los datos los necesitamos para realizar el algoritmo de *backpropagation*, por lo que se requiere guardarlos. Igualmente, no es eficiente calcular todos los pasos dato por dato, ya que eso lo haría muy, pero muy lento. Así que vamos a aporvechar que los datos vienen en forma de un *ndarray* de numpy y haremos todos los calculos en forma matricial tal como los vimos en clases. \n",
    "\n",
    "Sea $X$ la matriz de valores de entrada, entonces $A^{0} = X^T$ es una lista de vectores columna donde cada columna es $a^{(0)}$ para el objeto correspondiente. Así, los calculos se pueden realizar columna por columna, y simplemente\n",
    "$$ \n",
    "Z^{(l)} = W^{(l)}A_e^{(l-1)},\n",
    "$$\n",
    "donde $A_e^{(l-1)}$ es $A^{(l-1)}$ agregandole un 1 al inicio de cada vector (o lo que es lo mismo, agregandole un renglon de unos al inicio). Si procedemos de esta forma, entonces es importante recordar que al final $\\hat{Y} = (A^{(L)})^T$, ya que para la salida cada renglon es un dato diferente (de acuerdo a nuestra convención desde los otros algoritmos que hamos utilizado), mientras que internamente,para la red neuronal, cada columna proviene de un objeto diferente.\n",
    "\n",
    "Por último, es importante recordar que la normalización es muy importante para las redes neuronales, especialmente si se utiliza el método de descenso de gradiente, por lo que es importante normalizar los datos antes de que sean utilizados, con la información de normalización que se conoce. Esta normalización, con el fin que sea más rápida se asume se realiza *antes* de ingresar al algoritmo de *feedforward*, por lo que se asume que los datos vienen normalizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2. Completa el código de la función de *feedforward* para una red neuronal ya establecida (20 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extendida(matriz):\n",
    "    \"\"\"\n",
    "    Agrega un renglon de unos a una matriz\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.r_[np.ones((1, matriz.shape[1])), matriz]\n",
    "\n",
    "def feedforward(X, red_neuronal):\n",
    "    \"\"\"\n",
    "    Calcula la salida estimada para los valores de `X` utilizando red_neuronal\n",
    "    \n",
    "    @param X: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    @param red_neuronal: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal``\n",
    "    \n",
    "    @return: `lista_A`, donde `Y_est` es un ndarray de shape (T, k) donde T es el número de objetos y k es \n",
    "             el número de salidas (neuronas de salida de la red neuronal).\n",
    "             \n",
    "    \"\"\"    \n",
    "    # Inicializar A^{(0)}\n",
    "    # ----------Agragar código aqui -----------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Despues vamos a hacer lo propio por cada capa hasta antes de la última\n",
    "    for wl in red_neuronal['W'][:-1]:         \n",
    "        # Calcula A_e^{l-1}, Z^{(l)}, A^{l} y agrega a lista_A.        \n",
    "        # (puede ser todo junto o por partes)\n",
    "        # ----------Agragar código aqui -----------------\n",
    "        lista_A.append(logistica(wl.dot(extendida(lista_A[-1]))))\n",
    "        \n",
    "    # Calcula A^{L} y agrega a lista_A de acuerdo al tipo de salida\n",
    "    # ----------Agragar código aqui -------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return lista_A\n",
    "                                      \n",
    "    \n",
    "    \n",
    "def prueba_feedforward():\n",
    "    \"\"\"\n",
    "    Función para validar la función de fedforward \n",
    "    \n",
    "    Se inicializa una red de dos capas (más capa de entrada) y se \n",
    "    imponen los pesos. Esto se hizo a mano para las diferentes\n",
    "    entradas, así que se espera que la función de feedforward de\n",
    "    cosas similares\n",
    "\n",
    "    \"\"\"\n",
    "    # Inicializa red neuronal\n",
    "    rn = inicializa_red_neuronal(3, [2, 2, 1], 'lineal')\n",
    "    \n",
    "    # Modificamos pesos\n",
    "    rn['W'][0] = np.array([[0.5, -0.3, -0.7],\n",
    "                           [-1, 0.2, 0.3]])\n",
    "    rn['W'][1] = np.array([[0, 0.5, -0.5]])\n",
    "    \n",
    "    #Ponemos algunas entradas sencillas\n",
    "    x = np.array([[0,   0],\n",
    "                  [1,   1],\n",
    "                  [-1, -1]])\n",
    "    \n",
    "    # Y el valor de A calculado a mano\n",
    "    A1 = np.array([[0.6225, 0.3775, 0.8176],\n",
    "                   [0.2689, 0.3775, 0.1824]])\n",
    "    A2 = np.array([0.17676, 0, 0.3176])\n",
    "    \n",
    "    lista_A = feedforward(x, rn)\n",
    "    assert np.sum(np.abs(lista_A[1] - A1)) <= 0.001\n",
    "    assert np.sum(np.abs(lista_A[2] - A2)) <= 0.001\n",
    "    \n",
    "    #Ahora vamos a ver que pasa si cambiamos la salida\n",
    "    rn['tipo'] = 'logistica'\n",
    "    lista_A = feedforward(x, rn)\n",
    "    A2 = np.array([0.544075, 0.5, 0.5787])\n",
    "\n",
    "    assert np.sum(np.abs(lista_A[2] - A2)) <= 0.001\n",
    "    \n",
    "    return \"Paso la prueba\"\n",
    "\n",
    "print prueba_feedforward()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con  la función de feedforward desarrollada, entonces podemos hacer una función para calcular el costo final, la cual depende de las salidas `Y`, de las salidas estimadas por la red neuronal `Y_est`, y del tipo de salida. Igualmente, para agregar la regularización, es necesario un valor `lammbda` de regularización y los parámetros de la red neuronal (`lista_thetas` de la estructura de datos de la red neuronal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3: Completa el código de la función de costo (10 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costo(Y, Y_est, tipo):\n",
    "    \"\"\"\n",
    "    Calcula la función de costo de una red neuronal, de acuerdo al tipo de salida \n",
    "    \n",
    "    @param Y: un ndarray de dimension (T, K) con los valores de salida\n",
    "    @param Y_est: un ndarray de dimension (T, K) con los valores de salida estimados\n",
    "    @param tipo: Un string que puede ser 'lineal', 'logistica'o 'softmax'\n",
    "    \n",
    "    \"\"\"\n",
    "    # ----------Agragar código aqui -------------------------------\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def prueba_costo():\n",
    "    \"\"\"\n",
    "    La unidad de prueba de la función costo\n",
    "    \n",
    "    \"\"\"\n",
    "    Y = np.array([[.1, 2.2, 3.6, 0]]).T\n",
    "    Y_est = np.array([[.3, 2.0, 3.8, -0.2]]).T\n",
    "    assert abs(costo(Y, Y_est, 'lineal') - 0.02) < 1e-8\n",
    "    \n",
    "    Y = np.array([[1, 0, 1, 0]]).T\n",
    "    Y_est = np.array([[.9, 0.1, 0.8, 0.2]]).T\n",
    "    assert abs(costo(Y, Y_est, 'logistica') - 0.164252033486018) < 1e-8\n",
    "    \n",
    "    Y = np.array([[1, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]).T\n",
    "    Y_est = np.array([[0.8, 0.1, 0.1], \n",
    "                      [0.15, 0.8, 0.05],\n",
    "                      [0.9, 0.05, 0.05],\n",
    "                      [0.021, 0.079, 0.9]])\n",
    "    assert abs(costo(Y, Y_est, 'softmax') - 0.164252033486018) < 1e-8\n",
    "    \n",
    "    return 'paso la prueba'\n",
    "\n",
    "print prueba_costo()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculando el gradiente con el algoritmo de *Backpropagation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es este apartado se genera el gradiente utilizando simplemente el algoritmo de backpropagarion y se prueba con una red neuronal ya hecha anteriormente. Igualmente se realiza un método para el calculo en forma numérica del gradiente, con fines de comprobación que el algoritmo está funcionando correctamente.\n",
    "\n",
    "Primero vamos a desarrollar el método de *backpropagation*, el cual es el siguiente:\n",
    "\n",
    "1. Calcular $\\delta^{(L)}$ para todos los datos a partir de $A^{L}$, $Y$ y el tipo de salida\n",
    "\n",
    "2. Para $l$ de $L-1$ hasta 1 con decrementos de $-1$:\n",
    "\n",
    "    1. Calcular $\\delta^{(l)}$ a partir de $\\delta^{(l+1)}$, $W^{(l+1)}$ y $A^{(l)}$\n",
    "    2. Calcular la derivada en cada uno de los pesos como\n",
    "       $$\n",
    "       \\frac{\\partial J(W)}{w_{ij}^{(l)}} = \\frac{1}{card(CA)}\\sum_{\\forall (x,y) \\in CA} a_j^{(l-1)} \\delta_i^{(l)}\n",
    "       $$\n",
    "       \n",
    "El desarrollo del algoritmo y los pasos en forma matricial se vieron con detalle en clase, por lo que aqui solo se da un pequeñño bosquejo esperando que se programe correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4 Completa el código de la función de backpropagation (20 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quita_renglon = lambda matriz: matriz[1:,:]\n",
    "\n",
    "def backpropagation(Y, listaA, red_neuronal):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente de los pesos de una red neuronal\n",
    "    \n",
    "    @param Y: ndarray de shape (T, k) donde T es el número de ejemplos y k el número de salidas\n",
    "    @param listaA: Una lista de activaciones por capas, obtenidas por la función `feedforward`\n",
    "    @param red_neuronal: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal`\n",
    "    \n",
    "    @return: `lista_Grad`, una lista del gradiente por peso y por capa\n",
    "             \n",
    "    \"\"\"    \n",
    "    # Calcula Delta para la primer capa\n",
    "    delta = (listaA[-1] - Y.T)\n",
    "    \n",
    "    #Numero de ejemplos\n",
    "    N = Y.shape[0]\n",
    "    \n",
    "    #Calcula el gradiente de los pesos de la última capa\n",
    "    lista_Grad = []\n",
    "    lista_Grad.append(delta.dot(extendida(listaA[-2]).T) / N)\n",
    "    \n",
    "    # Despues vamos a hacer lo propio por cada capa hasta antes de la última\n",
    "    for (Wl, Ala, Alant) in zip(red_neuronal['W'][-1:0:-1], listaA[-2:0:-1], listaA[-3::-1]):         \n",
    "        # Calcula la delta para la capa anterior.        \n",
    "        # ----------Agragar código aqui -----------------\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Calcula el gradiente para los pesos de la capa anterior.        \n",
    "        # ----------Agragar código aqui -----------------\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Si utilizaste el método append, no olvides de hacer la lista al revés\n",
    "    lista_Grad.reverse()\n",
    "    return lista_Grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema es que este tipo de algoritmos a la hora de codificarlos es muy típico que se tengan errores, tanto de concepto como de codificación, por lo que es necesario aplicar pruebas para estar seguros que el algoritmo funciona correctamente. \n",
    "\n",
    "Para eso, vamos a programar una forma alternativa de calcular una aproximación del gradiente en forma numérica y altamente ineficiente. Por supuesto que esta función no nos ayuda para utilizarla dentro de un método de optimización pero nos sirve para checar errores que podríamos haber ingresado en nuestro algoritmo.\n",
    "\n",
    "El método se basa en la idea que\n",
    "\n",
    "$$\n",
    "  \\left.\\frac{\\partial f(x)}{\\partial x}\\right|_{x = x_0} \\approx \\frac{f(x_0 + \\epsilon) - f(x_0 - \\epsilon)}{2 \\epsilon},  \n",
    "$$\n",
    "y esto se realiza para cada uno de los pesos para ir calculando los valores del gradiente. Esto no es nada eficiente pero es un método para poder validar el algoritmo de *backpropagation*, Recuerda que mientras más complejo, y más sensible a errores es un algoritmo, mejores deben de ser las técnicas para probar su correcto funcionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5 Completa el código de la función de gradiente numérico (10 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_numerico(X, Y, red_neuronal, epsilon):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente numérico para efectos de prueba del algoritmo de backpropagation.\n",
    "    Este algortimo se realiza expresamente de manera ineficiente pero clara, ya que su\n",
    "    funcionamiento debe de ser lo más claro posible.\n",
    "    \n",
    "    @param X: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    @param Y: ndarray de shape (T, k) donde k es el número de salidas\n",
    "    @param red_neuronal: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal``\n",
    "    @param epsilon: Un número flotante positivo, típicamente muy pequeño\n",
    " \n",
    "    @return: Una lista de gradientes de red_neuronal['W'] con la misma estructura y dimensión\n",
    "    \"\"\"\n",
    "    \n",
    "    # inicializa los gradientes a cero\n",
    "    rn = red_neuronal['W'][:]\n",
    "    listaGrad = [np.ones_like(Wl) for Wl in rn]\n",
    "    \n",
    "    for (l, Wl) in enumerate(rn):  #  Por cada capa\n",
    "        for i in range(Wl.shape[0]):  #  Por cada renglon\n",
    "            for j in range(Wl.shape[1]):  #  Por cada columna\n",
    "                # -------------------------------------------\n",
    "                # Insertar código aquí\n",
    "                # -------------------------------------------\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # --------------------------------------------\n",
    "    return listaGrad           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer una función de prueba utilizando un conjunto de datos reales (o un subconjunto de estos), y lo vamos a hacer para muchas posibles reinicializaciones de pesos. Este código va a servir para corregir ambos algoritmos de calculo de gradiente. Favor de no seguir más allá en la programación de la red neuronal hasta estar seguro que esto funcione correctamente. Para esto vamos a utilizar una base de datos ya conocida, la de dígitos, utilizada en la libreta de regresión softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Datos a utilizar para la prueba\n",
    "# para no tener que estarlos cargando de nuevo cada vez\n",
    "\n",
    "# Vamos a utilizar un subconkunto de datos y de atributos\n",
    "# para que pueda funcionar el gradiente numérico en un tiempo \n",
    "# aceptable\n",
    "\n",
    "\n",
    "data = np.load(\"digitos.npz\")\n",
    "x_prueba = data['X_entrena'][:100,:10]  # Solo 100 datos y 10 parámetros\n",
    "y_prueba = data['T_entrena'][:100,:]    # Todas las clases de los primeros 100 datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_gradiente(x, y):\n",
    "    \"\"\"\n",
    "    Unidad de prueba de backpropagation\n",
    "    \n",
    "    Utiliza la base de datos de digitos, y pueba con varias capas y varios numeros de neuronas\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    n0 = x.shape[1]\n",
    "    nL = y.shape[1]\n",
    "    \n",
    "    for ocultas in [1, 2, 3]: \n",
    "        for neuronas in [1, 2]:\n",
    "            for tipo in ['lineal', 'logistica', 'softmax']:\n",
    "                for prueba in range(5):\n",
    "                    # print \"Tipo: \", tipo, \", capas ocultas: \", ocultas, \", neuronas en capas ocultas: \", neuronas * n0\n",
    "                    rn = inicializa_red_neuronal(ocultas + 2, \n",
    "                                                 [n0] + (ocultas * [neuronas * n0]) + [nL],\n",
    "                                                 tipo)\n",
    "                    listaA = feedforward(x, rn)\n",
    "                    listaGrad = backpropagation(y, listaA, rn)\n",
    "                    listaGradnum = gradiente_numerico(x, y, rn, 1e-3)\n",
    "                    diferencias = [np.abs(G - Gn).max() for (G, Gn) in zip(listaGrad, listaGradnum)]\n",
    "                    assert max(diferencias) < 1e-3\n",
    "    return \"Paso la prueba\"\n",
    "\n",
    "print prueba_gradiente(x_prueba, y_prueba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aprendizaje con descenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se realiza el decenso de gradiente, incluidos\n",
    "\n",
    "1. Inercia\n",
    "2. Parada temprana\n",
    "\n",
    "y por último se prueba en un problema sencillo (el problema de los dígitos).\n",
    "\n",
    "Para esto vamos primero a desarrollar el algoritmo de descenso de gradiente, tal como lo vimos en clase, esto es, agregandole el término de inercia, para evitar caer en mínimos locales fácilmente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 6 Completa el código de la función de descenso de gradiente con inercia (10 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def des_grad(x, y, red_neuronal, alpha=0.1, gamma=0.9, max_epoch=100):\n",
    "    \"\"\"\n",
    "    Descenso de gradiente con inercia, utilizando backpropagation. Se asumen los datos ya normalizados\n",
    "    \n",
    "    @param x: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    @param y: ndarray de shape (T, k) donde k es el número de salidas\n",
    "    @param red_neuronal: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal``\n",
    "    @param alpha: Paso de aprendizaje, por default 0.1\n",
    "    @param gamma: Factor de olvido para el computo de la inercia \n",
    "    @param max_epoch: Máximo número de epoch (iteraciones) por default 100\n",
    "    \n",
    "    @return: True si se hizo el aprendizaje (modifica red_neuronal por referencia)\n",
    "\n",
    "    \"\"\"\n",
    "    lista_Inc = [np.zeros_like(Wl) for Wl in red_neuronal['W']]\n",
    "    for epoch in xrange(max_epoch):\n",
    "        lista_A = feedforward(x, red_neuronal)\n",
    "        lista_Grad = backpropagation(y, lista_A, red_neuronal)\n",
    "        for (l, Grad) in enumerate(lista_Grad):\n",
    "            # -------------------------------------------\n",
    "            # Insertar código aquí\n",
    "            # -------------------------------------------\n",
    "\n",
    "                \n",
    "                \n",
    "            # --------------------------------------------\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y a partir de esta función, pues se puede construir la funcion necesaria para hacer parada temprana (*early stopping*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 7 Completa el código de la función de parada temprana (10 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def early_stop(x, y, red_neuronal, alpha=0.1, gamma=0.9, \n",
    "               inter_epoch=100, max_epoch=1000, porcentaje=0.8, criterio=0.1):\n",
    "    \"\"\"\n",
    "    Parada temprana como función envolvente de des_grad\n",
    "\n",
    "    @param x: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    @param y: ndarray de shape (T, k) donde k es el número de salidas\n",
    "    @param red_neuronal: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal`\n",
    "    @param alpha: Paso de aprendizaje, por default 0.1\n",
    "    @param gamma: Factor de olvido para el computo de la inercia \n",
    "    @param inter_epoch: Máximo número de epoch (iteraciones) antes de checar por el sobreaprendizaje por default 100\n",
    "    @param max_epoch: Máximo número de epoch (iteraciones) por default 1000\n",
    "    @param porcentaje: Porcentaje de datos usados para aprendizaje (el resto para validación) por default 0.8 (80%).\n",
    "    @param criterio: Minimo valor para considerar sobreaprendizaje (default 0.1)\n",
    "\n",
    "    @return: True si se hizo el aprendizaje (modifica red_neuronal por referencia) sin parada temprana,\n",
    "             si hubo parada temprana devuelve el número de epoch realizados\n",
    "    \n",
    "    \"\"\"\n",
    "    limite = int(x.shape[0] * porcentaje) \n",
    "    indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    xt, xv = x[indices[:limite], :], x[indices[limite:], :] \n",
    "    yt, yv = y[indices[:limite], :], y[indices[limite:], :] \n",
    "    \n",
    "    anterior = 1e10\n",
    "    for periodos in xrange(max_epoch/inter_epoch):\n",
    "        # -------------------------------------------\n",
    "        # Insertar código aquí\n",
    "        # -------------------------------------------\n",
    "\n",
    "                \n",
    "                \n",
    "        # --------------------------------------------\n",
    "\n",
    "    return True   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último podemos hacer dos funciones envolventes sencillas, \n",
    "una de aprendizaje y otra de predicción, donde ya se incluya todo junto. Estas funciones son provistas.\n",
    "\n",
    "Tomese en cuenta que la función de aprendizaje toma en cuenta que se empieza el aprendizaje de nada (desde inicializar\n",
    "la red neuronal), y que contempla reinicios aleatorios (por default 5). Para otras cosas es necesario cambiar las funciones aqui presentadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def aprende(x, y, tipo, capas_ocultas, neuronas_en_capas_ocultas,\n",
    "            alpha=0.1, gamma=0.9, inter_epoch=100, max_epoch=1000, \n",
    "            porcentaje=0.8, criterio=0.1, reinicios=5):\n",
    "    \"\"\"\n",
    "    @param x: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    @param y: ndarray de shape (T, k) donde k es el número de salidas\n",
    "    @param tipo: Tipo de salida de la red neurnal (in ['lineal', 'logistica', 'softmax'])\n",
    "    @param capas_ocultas: Número de capas ocultas\n",
    "    @param neuronas_en_capas_ocultas: Número de neuronas en cada capa oculta\n",
    "    @param alpha: Paso de aprendizaje, por default 0.1\n",
    "    @param gamma: Factor de olvido para el computo de la inercia \n",
    "    @param inter_epoch: Máximo número de epoch (iteraciones) antes de checar por el sobreaprendizaje por default 100\n",
    "    @param max_epoch: Máximo número de epoch (iteraciones) por default 1000\n",
    "    @param porcentaje: Porcentaje de datos usados para aprendizaje (el resto para validación) por default 0.8 (80%).\n",
    "    @param criterio: Minimo valor para considerar sobreaprendizaje (default 0.1)\n",
    "    @param reinicios: Número de reinicios aleatorios\n",
    "    \n",
    "    return: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal``\n",
    "    \n",
    "    \"\"\"\n",
    "    mejor = 1e20\n",
    "    mu, std = obtiene_medias_desviaciones(x)\n",
    "    xn = normaliza(x, mu, std)\n",
    "    \n",
    "    for _ in range(reinicios):\n",
    "        rn = inicializa_red_neuronal(2 + capas_ocutas, \n",
    "                                     [x.shape[1]] + \n",
    "                                      capas_ocultas * [neuronas_en_capas_ocultas] + \n",
    "                                      [y.shape[1]], \n",
    "                                     tipo)\n",
    "        rn['medias'] = mu\n",
    "        rn['std'] = std\n",
    "        \n",
    "        early_stop(xn, y, rn, alpha, gamma, inter_epoch, max_epoch, porcentaje, criterio)\n",
    "        y_est = feedforward(xn, rn)[-1].T\n",
    "        actual = costo(y, y_est, rn['tipo'])\n",
    "        if actual < mejor:\n",
    "            red_neuronal = deepcopy(rn)\n",
    "            mejor = actual\n",
    "    return red_neuronal\n",
    "\n",
    "\n",
    "def predice(x, red_neuronal):\n",
    "    \"\"\"\n",
    "    Devuelve la prediccion de una red neuronal, solo una función envolvente de feedforward\n",
    "\n",
    "    @param x: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    @param red_neuronal: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal`\n",
    "\n",
    "    \"\"\"\n",
    "    xn = normaliza(x, red_neuronal['medias'], red_neuronal['std'])\n",
    "    y_est = feedforward(xn, red_neuronal)[-1].T\n",
    "    if red_neuronal['tipo'] in ['logistica', 'softmax']:\n",
    "        t = np.zeros_like(y_est)\n",
    "        t[np.arange(y_est.shape[0]), y_est.argmax(axis=1)] = 1\n",
    "        return t        \n",
    "    return y_est \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora probamos la red neuronal con el problema de los dígitos. en este caso, lo interesante es escoger los parámetros para obtener resultados decentes en un tiempo decente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 8 Completa el código para la predicción en el problema de los digitos (10 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"digitos.npz\")\n",
    "\n",
    "x = data['X_entrena']\n",
    "y = data['T_entrena']\n",
    "x_prueba = data['X_valida']\n",
    "y_prueba = data['T_valida']\n",
    "\n",
    "# Aqui hay que escoger los parámetros de la red\n",
    "# -----------------------------------------------\n",
    "tipo = \n",
    "capas_ocultas =  \n",
    "neuronas_en_capas_ocultas = \n",
    "alpha = \n",
    "gamma = \n",
    "inter_epoch = \n",
    "max_epoch = \n",
    "porcentaje = \n",
    "criterio = \n",
    "reinicios = \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Esto puede llegar a tardar MUCHO\n",
    "red_neuronal = aprende(x, y, tipo, capas_ocultas, neuronas_en_capas_ocultas,\n",
    "            alpha, gamma, inter_epoch, max_epoch, porcentaje, criterio, reinicios)\n",
    "\n",
    "y_est = predice(x_prueba, red_neuronal)\n",
    "\n",
    "correctos = np.where(np.abs((y_prueba - y_est)).sum(axis=1) == 0, 1, 0).mean()\n",
    "\n",
    "print \"El porcentaje de valores bien clasificados en el conjunto de prueba es de %2.2f %%\" %(100 * correctos)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
